import os
import requests
from bs4 import BeautifulSoup
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
import time
import re
import copy 
from datetime import datetime, timedelta
import dateparser 
import json


# =========================================================
# ржХржиржлрж┐ржЧрж╛рж░рзЗрж╢ржи рж╕рзЗржЯрж┐ржВрж╕ (Configuration Settings)
# =========================================================

TARGET_LISTING_URL = os.getenv('TARGET_URL')
BLOG_ID = os.getenv('BLOG_ID')

SCOPES = ['https://www.googleapis.com/auth/blogger']
MAX_POSTS_TO_LOAD = int(os.getenv('MAX_POSTS', 50))
POST_DELAY_SECONDS = 10 
DELETE_DELAY_SECONDS = 1
SCRAPED_POST_TAG = os.getenv('POST_TAG', 'ржЕржирзНржпрж╛ржирзНржп')

# ЁЯОп ржирждрзБржи ржбрзЗржЯ ржЯрзНржпрж╛ржЧ ржкрзНржпрж╛ржЯрж╛рж░рзНржи
WEB_END_DATE_TAG_PREFIX = 'WebEndDate:'

# =========================================================
# рж╕рж╣рж╛ржпрж╝ржХ ржлрж╛ржВрж╢ржи: API ржЕржирзБржорзЛржжрж┐ржд рж╕рж╛рж░рзНржнрж┐рж╕ ржЕржмржЬрзЗржХрзНржЯ рждрзИрж░рж┐
# =========================================================

def get_blogger_service():
    """Google Blogger API-ржПрж░ ржЬржирзНржп ржорзЗржорзЛрж░рж┐ ржерзЗржХрзЗ ржХрзНрж░рзЗржбрзЗржирж╢рж┐рзЯрж╛рж▓ рж▓рзЛржб ржХрж░рзЗред"""
    creds = None
    
    google_token_json = os.environ.get('GOOGLE_TOKEN')
    google_creds_json = os.environ.get('GOOGLE_CREDENTIALS')

    if google_token_json:
        token_info = json.loads(google_token_json)
        creds = Credentials.from_authorized_user_info(token_info, SCOPES)

    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            if google_creds_json:
                secret_info = json.loads(google_creds_json)
                flow = InstalledAppFlow.from_client_config(secret_info, SCOPES)
                creds = flow.run_local_server(port=0)

    return build('blogger', 'v3', credentials=creds)

# =========================================================
# ЁЯФД ржзрж╛ржк рзй.рзз: ржмрж┐ржжрзНржпржорж╛ржи ржкрзЛрж╕рзНржЯ ржЯрж╛ржЗржЯрзЗрж▓ рж╕ржВржЧрзНрж░рж╣ (рж╕ржХрж▓)
# =========================================================

def get_existing_titles(service, blog_id):
    """ржмрзНрж▓ржЧ ржерзЗржХрзЗ SCRAPED_POST_TAG ржпрзБржХрзНржд ржкрзЛрж╕рзНржЯрзЗрж░ ржмрж░рзНрждржорж╛ржи ржЯрж╛ржЗржЯрзЗрж▓ржЧрзБрж▓рж┐рж░ рж╕рзЗржЯ рж╕ржВржЧрзНрж░рж╣ ржХрж░рзЗред"""
    print("        ЁЯФН ржбрзБржкрзНрж▓рж┐ржХрзЗрж╢ржи ржЪрзЗржХ: ржмрзНрж▓ржЧрзЗрж░ рж╕ржХрж▓ 'ржЕржирзНржпрж╛ржирзНржп' ржЯрзНржпрж╛ржЧржпрзБржХрзНржд ржкрзЛрж╕рзНржЯ ржЯрж╛ржЗржЯрзЗрж▓ рж▓рзЛржб ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ...")
    existing_titles = set()
    try:
        # рж╢рзБржзрзБржорж╛рждрзНрж░ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЯрзНржпрж╛ржЧ ржпрзБржХрзНржд ржкрзЛрж╕рзНржЯржЧрзБрж▓рж┐ ржлрж┐рж▓рзНржЯрж╛рж░ ржХрж░рж╛
        response = service.posts().list(
            blogId=blog_id, 
            labels=SCRAPED_POST_TAG, 
            fetchBodies=False, 
            maxResults=500 
        ).execute()
        
        posts = response.get('items', [])
        for post in posts:
            existing_titles.add(post['title'])
            
        # ЁЯМЯ ржЯрж╛рж░рзНржорж┐ржирж╛рж▓рзЗ рж╕рж░рзНржмрж╢рзЗрж╖ ржЯрж╛ржЗржЯрзЗрж▓ ржкрзНрж░рж┐ржирзНржЯ ржХрж░рж╛ 
        latest_title = posts[0].get('title', 'ржХрзЛржирзЛ ржкрзЛрж╕рзНржЯ ржирзЗржЗ') if posts else 'ржХрзЛржирзЛ ржкрзЛрж╕рзНржЯ ржирзЗржЗ'
        print(f"        тЬЕ ржмрзНрж▓ржЧрзЗ '{SCRAPED_POST_TAG}' ржЯрзНржпрж╛ржЧржпрзБржХрзНржд ржмрж┐ржжрзНржпржорж╛ржи ржкрзЛрж╕рзНржЯ ржкрж╛ржУржпрж╝рж╛ ржЧрзЗржЫрзЗ: {len(existing_titles)} ржЯрж┐ред")
        print(f"        тД╣я╕П ржЖржкржирж╛рж░ ржмрзНрж▓ржЧрзЗрж░ рж╕рж░рзНржмрж╢рзЗрж╖ ржкрзЛрж╕рзНржЯрзЗрж░ ржЯрж╛ржЗржЯрзЗрж▓ (ржЪрзЗржХрзЗрж░ ржЬржирзНржп): **{latest_title}**")
        
    except Exception as e:
        print(f"        тЭМ ржмрж┐ржжрзНржпржорж╛ржи ржкрзЛрж╕рзНржЯ рж▓рзЛржб ржХрж░рж╛рж░ рж╕ржоржпрж╝ рждрзНрж░рзБржЯрж┐: {e}")
        
    return existing_titles

# =========================================================
# ЁЯЪА ржзрж╛ржк рзй: ржкрзЛрж╕рзНржЯрзЗрж░ рждрж╛рж▓рж┐ржХрж╛ рж╕ржВржЧрзНрж░рж╣ ржУ рждрж╛рж░рж┐ржЦ ржлрж┐рж▓рзНржЯрж╛рж░рж┐ржВ
# =========================================================

def get_all_post_links_and_details(listing_url):
    """ржЖрж░рзНржХрж╛ржЗржн ржкрзЗржЬ ржерзЗржХрзЗ ржкрзЛрж╕рзНржЯрзЗрж░ URL, рж╢рж┐рж░рзЛржирж╛ржо, ржПржмржВ ржбрзЗржЯрж▓рж╛ржЗржи рж╕ржВржЧрзНрж░рж╣ ржХрж░рзЗ ржПржмржВ ржлрж┐рж▓рзНржЯрж╛рж░ ржХрж░рзЗред"""
    print(f"\nтЦ╢я╕П ржзрж╛ржк рзй: ржкрзЛрж╕рзНржЯрзЗрж░ рждрж╛рж▓рж┐ржХрж╛ рж╕ржВржЧрзНрж░рж╣ рж╢рзБрж░рзБ рж╣ржЪрзНржЫрзЗ: {listing_url}")
    today = datetime.now().date()
    
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(listing_url, headers=headers, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"тЭМ ржкрзЛрж╕рзНржЯ рждрж╛рж▓рж┐ржХрж╛ рж░рж┐ржХрзЛржпрж╝рзЗрж╕рзНржЯ ржмрзНржпрж░рзНрже рж╣ржпрж╝рзЗржЫрзЗ: {e}")
        return []

    soup = BeautifulSoup(response.text, 'lxml')
    all_target_details = []

    # рж╢рзБржзрзБржорж╛рждрзНрж░ ржкрзЛрж╕рзНржЯ рж▓рж┐ржЩрзНржХ ржЯрж╛рж░рзНржЧрзЗржЯ ржХрж░рж╛
    all_links = soup.find_all('a', href=re.compile(r'/\d{4}/\d{2}/')) 

    for a_tag in all_links:
        post_url = a_tag.get('href')
        r_snippetized_div = a_tag.find('div', class_='r-snippetized')

        if r_snippetized_div:
            snippet_body_tag = r_snippetized_div.find('div', class_='snippet-body')
            deadline_text = snippet_body_tag.text.strip() if snippet_body_tag else ""

            temp_r_snippetized = copy.copy(r_snippetized_div)
            
            # snippet-body ржбрж┐ржХржорзНржкрзЛржЬ ржХрж░рж╛ 
            if temp_r_snippetized.find('div', class_='snippet-body'):
                temp_r_snippetized.find('div', class_='snippet-body').decompose()
            
            title = temp_r_snippetized.text.strip()
            
            if 'blogspot.com/' in post_url and len(title) > 5:
                            
                is_deadline_post = re.search(r'deadline|рж╕ржоржпрж╝рж╕рзАржорж╛', deadline_text, re.IGNORECASE)
                is_result_post = 'ржЪрзВржбрж╝рж╛ржирзНржд ржлрж▓рж╛ржлрж▓' in deadline_text
                
                post_type = None
                
                # A. рж╢рзБржзрзБржорж╛рждрзНрж░ ржбрзЗржбрж▓рж╛ржЗржи ржкрзЛрж╕рзНржЯрзЗрж░ ржбрзЗржЯ ржЪрзЗржХ ржХрж░рж╛ рж╣ржмрзЗ
                if is_deadline_post:
                    post_type = 'deadline'
                    
                    # ЁЯОп ржбрзЗржбрж▓рж╛ржЗржи ржбрзЗржЯ ржПржХрзНрж╕ржЯрзНрж░рзНржпрж╛ржХрзНржЯ ржХрж░рж╛
                    match = re.search(r'(?:Deadline|рж╕ржоржпрж╝рж╕рзАржорж╛)(?:[:\s]+)?\s*(\d{1,2}\s+[A-Za-z]{3,}\s+\d{4})', deadline_text, re.IGNORECASE)
                    
                    if match:
                        date_str = match.group(1)
                        # ржбрзЗржЯ ржкрж╛рж░рзНрж╕ ржХрж░рж╛рж░ ржЬржирзНржп ржмрж╛ржВрж▓рж╛ ржорж╛рж╕рзЗрж░ ржирж╛ржо рж╕рж╣ ржПржирж╛рж▓рж┐ржЯрж┐ржХрзНрж╕ ржпрзБржХрзНржд ржХрж░рж╛
                        parsed_date = dateparser.parse(date_str, languages=['en', 'bn'])
                        
                        if parsed_date:
                            post_date = parsed_date.date()
                            
                            # ЁЯЫС ржлрж┐рж▓рзНржЯрж╛рж░рж┐ржВ: ржбрзЗржбрж▓рж╛ржЗржи ржЖржЬржХрзЗрж░ ржмрж╛ рждрж╛рж░ ржкрж░рзЗрж░ ржжрж┐ржи рж╣рждрзЗ рж╣ржмрзЗ
                            if post_date >= today:
                                all_target_details.append({
                                    'title': title, 
                                    'url': post_url, 
                                    'deadline_text': deadline_text,
                                    'type': post_type,
                                    'parsed_date': parsed_date 
                                })
                            else:
                                print(f"        тЭМ ржбрзЗржЯ ржлрж┐рж▓рзНржЯрж╛рж░: {title} ржмрж╛ржж ржжрзЗржУржпрж╝рж╛ рж╣рж▓рзЛред (ржбрзЗржбрж▓рж╛ржЗржи: {post_date})")
                                continue
                        else:
                            print(f"        тЪая╕П ржбрзЗржЯ ржкрж╛рж░рзНрж╕рзЗ ржмрзНржпрж░рзНрже: {title} ржмрж╛ржж ржжрзЗржУржпрж╝рж╛ рж╣рж▓рзЛред")
                            continue
                            
                # B. ржлрж▓рж╛ржлрж▓ ржкрзЛрж╕рзНржЯ
                elif is_result_post:
                    post_type = 'result'
                    all_target_details.append({
                        'title': title, 
                        'url': post_url, 
                        'deadline_text': deadline_text,
                        'type': post_type,
                        'parsed_date': None 
                    })
                    
                # C. ржЕржирзНржпрж╛ржирзНржп ржкрзЛрж╕рзНржЯ ржмрж╛ржж ржжрзЗржУржпрж╝рж╛ 
                else:
                    print(f"        тЪая╕П ржЯрж╛ржЗржк ржлрж┐рж▓рзНржЯрж╛рж░: {title} ржмрж╛ржж ржжрзЗржУржпрж╝рж╛ рж╣рж▓рзЛ (ржбрзЗржбрж▓рж╛ржЗржи ржмрж╛ ржлрж▓рж╛ржлрж▓ ржиржпрж╝)ред")

    print(f"тЬЕ ржкрзЛрж╕рзНржЯрзЗрж░ рждрж╛рж▓рж┐ржХрж╛ рж╕ржВржЧрзНрж░рж╣ ржУ рждрж╛рж░рж┐ржЦ ржлрж┐рж▓рзНржЯрж╛рж░рж┐ржВ рж╕ржорзНржкржирзНржи рж╣ржпрж╝рзЗржЫрзЗред ржнрзНржпрж╛рж▓рж┐ржб ржкрзЛрж╕рзНржЯ ржкрж╛ржУржпрж╝рж╛ ржЧрзЗржЫрзЗ: {len(all_target_details)} ржЯрж┐")
    
    final_list = all_target_details[:MAX_POSTS_TO_LOAD] 
    return final_list


# =========================================================
# ржзрж╛ржк рзи: рж╕рж┐ржЩрзНржЧрзЗрж▓ ржкрзЛрж╕рзНржЯ ржерзЗржХрзЗ ржЗржорзЗржЬ/ржЯрзНржпрж╛ржЧ/рж▓рж┐ржЩрзНржХ ржирж┐рж╖рзНржХрж╛рж╢ржи (ржЖржкржбрзЗржЯрзЗржб)
# =========================================================

def scrape_single_post_media(post_url):
    """ржПржХржЯрж┐ ржПржХржХ ржмрзНрж▓ржЧ ржкрзЛрж╕рзНржЯ URL ржерзЗржХрзЗ ржЗржорзЗржЬ, рж▓рзЗржмрзЗрж▓ ржПржмржВ ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХ (рж╕рзНржорж╛рж░рзНржЯ ржлрж▓ржмрзНржпрж╛ржХ рж╕рж╣) ржмрзЗрж░ ржХрж░рзЗ ржЖржирзЗред"""
    print(f"        ЁЯФД ржзрж╛ржк рзи: ржорж┐ржбрж┐ржпрж╝рж╛ ржУ рж▓рж┐ржВржХ ржбрзЗржЯрж╛ рж╕ржВржЧрзНрж░рж╣ рж╢рзБрж░рзБ: {post_url[-40:]}...")
    media_data = {'images': [], 'download_links': [], 'labels': [], 'application_link': None, 'application_text': None} 
    
    try:
        response = requests.get(post_url, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"        тЭМ ржПржХржХ ржкрзЛрж╕рзНржЯ рж░рж┐ржХрзЛржпрж╝рзЗрж╕рзНржЯ ржмрзНржпрж░рзНрже рж╣ржпрж╝рзЗржЫрзЗ: {e}")
        return media_data

    soup = BeautifulSoup(response.text, 'html.parser') 
    
    # A. ржкрзЛрж╕рзНржЯ ржмржбрж┐ ржХржирзНржЯрзЗржЗржирж╛рж░ ржЦрзЛржБржЬрж╛
    post_body = soup.find('div', class_='post-body') 
    if not post_body:
        post_body = soup.find('div', class_='entry-content')
    if not post_body:
        return media_data

    # B. ржЗржорзЗржЬ рж╕ржВржЧрзНрж░рж╣
    images = post_body.select('div.separator img[src], div.separator img[data-src]')
    if not images:
        images = post_body.select('img[src], img[data-src]') 

    if images:
        for img_tag in images:
            img_src = img_tag.get('src') or img_tag.get('data-src')
            if img_src:
                media_data['images'].append(img_src.replace('/s16000/', '/s1000/')) 
    else:
        print("        тЭМ WARNING: ржХрзЛржирзЛ ржЗржорзЗржЬ ржЦрзБржБржЬрзЗ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ред")

    # ЁЯОп E. ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХ (Application Link) рж╕ржВржЧрзНрж░рж╣ - (ржЖржкржбрзЗржЯрзЗржб рж▓ржЬрж┐ржХ) ЁЯОп
    
    # рзз. ржкрзНрж░ржержо ржЪрзЗрж╖рзНржЯрж╛: рж╕рж░рж╛рж╕рж░рж┐ 'ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХржГ' ржЯрзЗржХрзНрж╕ржЯ ржЦрзЛржБржЬрж╛ (ржмрж╛ржВрж▓рж╛)
    p_tags = post_body.find_all('p')
    for p_tag in p_tags:
        if 'ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХржГ' in p_tag.text:
            link_tag = p_tag.find('a', href=True)
            if link_tag:
                media_data['application_link'] = link_tag['href']
                media_data['application_text'] = p_tag.text.strip()
                break 

    # рзи. ржжрзНржмрж┐рждрзАрзЯ ржЪрзЗрж╖рзНржЯрж╛: ржпржжрж┐ ржЙржкрж░рзЗ ржирж╛ ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯ, ржЗржВрж░рзЗржЬрж┐ 'Apply' рж╢ржмрзНржжржЯрж┐ ржЦрзЛржБржЬрж╛
    if not media_data['application_link']:
        # рж╕ржм рж▓рж┐ржВржХ ржЯрзНржпрж╛ржЧ ржЦрзЛржБржЬрж╛
        all_links = post_body.find_all('a', href=True)
        
        for link in all_links:
            link_text = link.get_text().strip()
            # ржкрзНржпрж╛рж░рзЗржирзНржЯ ржПрж▓рж┐ржорзЗржирзНржЯрзЗрж░ ржЯрзЗржХрзНрж╕ржЯ (ржпрзЗржоржи: <p>Apply here: <a>Link</a></p>)
            parent_text = link.parent.get_text().strip() if link.parent else ""
            
            # ржХржирзНржбрж┐рж╢ржи рзз: рж▓рж┐ржВржХрзЗрж░ ржирж┐ржЬрзЗрж░ ржЯрзЗржХрзНрж╕ржЯрзЗ 'Apply' ржЖржЫрзЗ ржХрж┐ ржирж╛ (ржпрзЗржоржи: "Apply Now", "Click to Apply")
            if re.search(r'apply', link_text, re.IGNORECASE):
                media_data['application_link'] = link['href']
                media_data['application_text'] = "Apply Link: " + link_text
                print("        тД╣я╕П 'Apply' ржмрж╛ржЯржи/рж▓рж┐ржВржХ ржЯрзЗржХрзНрж╕ржЯ ржЦрзБржБржЬрзЗ ржкрж╛ржУржпрж╝рж╛ ржЧрзЗржЫрзЗред")
                break
            
            # ржХржирзНржбрж┐рж╢ржи рзи: рж▓рж┐ржВржХрзЗрж░ ржарж┐ржХ ржЖржЧрзЗрж░ ржмрж╛ ржкрзНржпрж╛рж░рзЗржирзНржЯ ржЯрзЗржХрзНрж╕ржЯрзЗ 'Apply' ржЖржЫрзЗ ржХрж┐ ржирж╛
            # (ржПржмржВ ржкрзНржпрж╛рж░рзЗржирзНржЯ ржЯрзЗржХрзНрж╕ржЯ ржЦрзБржм ржмрзЬ ржпрзЗржи ржирж╛ рж╣рзЯ, ржпрж╛рждрзЗ ржнрзБрж▓ рж▓рж┐ржВржХ ржирж╛ ржЖрж╕рзЗ)
            elif re.search(r'apply', parent_text, re.IGNORECASE) and len(parent_text) < 150:
                media_data['application_link'] = link['href']
                media_data['application_text'] = parent_text
                print("        тД╣я╕П 'Apply' ржЯрзЗржХрзНрж╕ржЯрзЗрж░ ржкрж╛рж╢рзЗ рж▓рж┐ржВржХ ржЦрзБржБржЬрзЗ ржкрж╛ржУржпрж╝рж╛ ржЧрзЗржЫрзЗред")
                break

    # C. ржЯрзНржпрж╛ржЧ/рж▓рзЗржмрзЗрж▓ рж╕ржВржЧрзНрж░рж╣
    labels_container = soup.find('span', class_='post-labels')
    if not labels_container:
        labels_container = soup.find('div', class_='post-footer') 
    
    if labels_container:
        label_tags = labels_container.select('a[rel="tag"]')
        if not label_tags:
            label_tags = labels_container.find_all('a') 
            
        if label_tags:
            media_data['labels'] = [tag.text.strip() for tag in label_tags if tag.text.strip()]
        
    # D. ржлрж▓ржмрзНржпрж╛ржХ ржЯрзНржпрж╛ржЧ
    if not media_data['labels']:
             media_data['labels'] = ['ржЬржм рж╕рж╛рж░рзНржХрзБрж▓рж╛рж░']
            
    link_status = 'ржкрж╛ржУржпрж╝рж╛ ржЧрзЗржЫрзЗ' if media_data['application_link'] else 'ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐'
    print(f"        тЬЕ ржорж┐ржбрж┐ржпрж╝рж╛ ржУ рж▓рж┐ржВржХ ржбрзЗржЯрж╛ рж╕ржВржЧрзНрж░рж╣ рж╕ржорзНржкржирзНржиред ржорзЛржЯ ржЗржорзЗржЬ: {len(media_data['images'])}, ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХ: {link_status}")
    return media_data

# =========================================================
# ржзрж╛ржк рзк: ржбрзБржкрзНрж▓рж┐ржХрзЗржЯ ржЪрзЗржХ ржУ ржкрзЛрж╕рзНржЯрж┐ржВ
# =========================================================

def scrape_filter_and_publish(listing_url, blogger_service, blog_id):
    """рж╕ржорж╕рзНржд ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╕ржоржирзНржмржпрж╝ ржХрж░рзЗред"""
    print("\n--- рж╕рзНржХрзНрж░рзНржпрж╛ржкрж┐ржВ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╢рзБрж░рзБ ---")
    
    # 1. ржЖржкржирж╛рж░ ржмрзНрж▓ржЧрзЗрж░ рж╕ржХрж▓ 'ржЕржирзНржпрж╛ржирзНржп' ржЯрзНржпрж╛ржЧржпрзБржХрзНржд ржкрзЛрж╕рзНржЯрзЗрж░ ржЯрж╛ржЗржЯрзЗрж▓ рж╕ржВржЧрзНрж░рж╣
    existing_titles = get_existing_titles(blogger_service, blog_id) 
    
    # 2. ржЯрж╛рж░рзНржЧрзЗржЯ рж╕рж╛ржЗржЯ ржерзЗржХрзЗ ржкрзЛрж╕рзНржЯрзЗрж░ рждрж╛рж▓рж┐ржХрж╛ рж╕ржВржЧрзНрж░рж╣
    all_target_details = get_all_post_links_and_details(listing_url) 
    
    if not all_target_details:
        print("ржкрзЛрж╕рзНржЯрзЗрж░ ржХрзЛржирзЛ рж▓рж┐ржВржХ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ред")
        return

    # 3. ржбрзБржкрзНрж▓рж┐ржХрзЗрж╢ржи ржЪрзЗржХ ржПржмржВ ржирждрзБржи ржкрзЛрж╕рзНржЯ ржлрж┐рж▓рзНржЯрж╛рж░ ржХрж░рж╛
    new_posts_to_publish = []
    
    for details in all_target_details:
        current_target_title = details['title']

        if current_target_title in existing_titles:
            print(f"тПня╕П ржзрж╛ржк рзк: рж╕рзНржХрж┐ржк ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ: **{current_target_title}** (ржбрзБржкрзНрж▓рж┐ржХрзЗржЯ)")
            continue
            
        print(f"\nтЦ╢я╕П ржзрж╛ржк рзк: ржирждрзБржи ржкрзЛрж╕рзНржЯ ржкрж╛ржУржпрж╝рж╛ ржЧрзЗржЫрзЗ ({details['type']}): {current_target_title}")
        
        media_data = scrape_single_post_media(details['url'])
        
        # ЁЯМЯ ржЗржорзЗржЬ ржлрж┐рж▓рзНржЯрж╛рж░
        if not media_data['images']:
            print(f"        тЭМ IMAGE FILTER: {current_target_title} ржП ржХрзЛржирзЛ ржЗржорзЗржЬ ржирзЗржЗ, ржкрзЛрж╕рзНржЯржЯрж┐ ржмрж╛ржж ржжрзЗржУржпрж╝рж╛ рж╣рж▓рзЛред")
            continue 

        # ЁЯОп ржбрж┐рж▓рж┐ржЯ ржбрзЗржЯ ржЧржгржирж╛рж░ рж▓ржЬрж┐ржХ
        delete_datetime = None
        
        if details['type'] == 'deadline' and details.get('parsed_date'):
            delete_datetime = details['parsed_date'] + timedelta(days=1)
            print(f"        тЬЕ ржбрзЗржбрж▓рж╛ржЗржи ржкрзЛрж╕рзНржЯрзЗрж░ ржбрж┐рж▓рж┐ржЯ ржбрзЗржЯ ржЧржгржирж╛ ржХрж░рж╛ рж╣рж▓рзЛ (ржкрж░рзЗрж░ ржжрж┐ржи): {delete_datetime.strftime('%Y-%m-%d')}")
        
        if delete_datetime is None:
            delete_datetime = datetime.now() + timedelta(days=7)
            print(f"        тЪая╕П ржлрж▓ржмрзНржпрж╛ржХ ржбрж┐рж▓рж┐ржЯ ржбрзЗржЯ ржЧржгржирж╛ ржХрж░рж╛ рж╣рж▓рзЛ (рзн ржжрж┐ржи ржкрж░): {delete_datetime.strftime('%Y-%m-%d')}")
            
        web_end_date_tag = f"{WEB_END_DATE_TAG_PREFIX}{delete_datetime.strftime('%d-%m-%Y')}"
        print(f"        ЁЯП╖я╕П WebEndDate ржЯрзНржпрж╛ржЧ рждрзИрж░рж┐: {web_end_date_tag}")

        # ржкрзЛрж╕рзНржЯ ржХржирзНржЯрзЗржирзНржЯ рждрзИрж░рж┐
        post_content = f"" 
        post_content += f"<p>ржбрзЗржбрж▓рж╛ржЗржи/ржлрж▓рж╛ржлрж▓ рждржерзНржп: {details['deadline_text']}</p>" 
        
        # ЁЯОп ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХ ржпрзБржХрзНржд ржХрж░рж╛ (ржпржжрж┐ ржерж╛ржХрзЗ)
        if media_data['application_link']:
            # ржЯрзЗржХрзНрж╕ржЯ ржХрзНрж▓рж┐ржи ржХрж░рж╛
            app_text = media_data['application_text'] if media_data['application_text'] else "ржЕржирж▓рж╛ржЗржирзЗ ржЖржмрзЗржжржи ржХрж░рзБржи"
            # 'ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХржГ' рж╢ржмрзНржжржЯрж┐ ржерж╛ржХрж▓рзЗ ржмрж╛ржж ржжрзЗржУрзЯрж╛, ржирж╛ ржерж╛ржХрж▓рзЗ ржпрж╛ ржЖржЫрзЗ рждрж╛ржЗ рж░рж╛ржЦрж╛
            app_text = app_text.replace('ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХржГ', '').strip()
            
            application_link = media_data['application_link']
            
            post_content += f'''
            <div style="border: 2px solid #4CAF50; padding: 15px; margin: 20px 0; border-radius: 8px; background-color: #f9fff9;">
                <p style="font-weight: bold; color: #333;">ржЖржмрзЗржжржирзЗрж░ рждржерзНржп:</p>
                <p style="margin-top: 5px;">{app_text}</p>
                <a href="{application_link}" target="_blank" 
                   style="display: inline-block; padding: 10px 20px; text-decoration: none; 
                          background-color: #f44336; color: white; border-radius: 5px; 
                          font-weight: bold; margin-top: 10px;">
                    тЮбя╕П ржЕржирж▓рж╛ржЗржирзЗ ржЖржмрзЗржжржи ржХрж░рзБржи
                </a>
            </div>
            '''
        else:
             print("        тЪая╕П ржЖржмрзЗржжржирзЗрж░ рж▓рж┐ржВржХ ржЦрзБржБржЬрзЗ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ред")
        
        # ржЗржорзЗржЬ ржПржмржВ ржбрж╛ржЙржирж▓рзЛржб рж▓рж┐ржЩрзНржХ ржпрзЛржЧ
        post_content += "<h3>рж╕ржВржпрзБржХрзНржд ржЫржмрж┐:</h3>"
        post_content += '<div style="text-align: center;">' 
        for i, img_src in enumerate(media_data['images']):
            img_src_s1000 = img_src.replace('/s16000/', '/s1000/')
            post_content += f'<img src="{img_src_s1000}" style="max-width:100%; height:auto; margin: 10px 0;" />'
            full_res_url = img_src.replace('/s1000/', '/s16000/') 
            button_text = f"Download (Image-{i+1})"
            post_content += f'''
            <a href="{full_res_url}" download="image_{i+1}" target="_blank" 
                style="display: block; margin: 10px auto; padding: 10px 20px; text-decoration: none; 
                        background-color: #4CAF50; color: white; border-radius: 5px; width: fit-content; font-weight: bold;">
                        {button_text}
            </a>
            '''
        post_content += '</div>'
        post_content += "<p>--- рждржерзНржпрж╕рзВрждрзНрж░: рж╕рж░ржХрж╛рж░рж┐ ржЪрж╛ржХрж░рж┐ ржкрзНрж░рж╕рзНрждрзБрждрж┐ ржЕрзНржпрж╛ржк ---</p>"
        
        final_labels = media_data.get('labels', [])
        if SCRAPED_POST_TAG not in final_labels:
            final_labels.append(SCRAPED_POST_TAG)
            
        final_labels.append(web_end_date_tag)

        new_posts_to_publish.append({
            'title': current_target_title,
            'content': post_content,
            'labels': final_labels
        })

    # 4. ржкрзЛрж╕рзНржЯрж┐ржВ
    posts_to_publish_in_order = new_posts_to_publish[::-1]
    print(f"\nтЮбя╕П ржзрж╛ржк рзк: ржорзЛржЯ **{len(posts_to_publish_in_order)}** ржЯрж┐ ржирждрзБржи ржкрзЛрж╕рзНржЯ ржкрзНрж░ржХрж╛рж╢рзЗрж░ ржЬржирзНржп ржкрзНрж░рж╕рзНрждрзБрждред")

    if posts_to_publish_in_order:
        published_titles = publish_posts(blogger_service, blog_id, posts_to_publish_in_order)
        
        if published_titles:
            print(f"\nЁЯОЙ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╕ржорзНржкржирзНржи! {len(published_titles)} ржЯрж┐ ржирждрзБржи ржкрзЛрж╕рзНржЯ рж╕ржлрж▓ржнрж╛ржмрзЗ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ржХрж░ржг ржУ ржкрзНрж░ржХрж╛рж╢рж┐ржд рж╣ржпрж╝рзЗржЫрзЗред")
    else:
        print("ржкрзЛрж╕рзНржЯ ржХрж░рж╛рж░ ржЬржирзНржп ржХрзЛржирзЛ ржирждрзБржи ржбрзЗржЯрж╛ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐ред")


# =========================================================
# ржзрж╛ржк рзл: publish_posts (ржкрж╛ржмрж▓рж┐ржХ ржкрзЛрж╕рзНржЯрж┐ржВ)
# =========================================================

def publish_posts(service, blog_id, posts_data):
    """рж╕ржВржЧрзНрж░рж╣ ржХрж░рж╛ ржкрзЛрж╕рзНржЯ ржбрзЗржЯрж╛ ржЖржкржирж╛рж░ ржмрзНрж▓ржЧрзЗ ржкрзНрж░ржХрж╛рж╢ ржХрж░рзЗред"""
    print("    ЁЯЪА ржзрж╛ржк рзл: ржмрзНрж▓ржЧрж╛рж░рзЗ ржкрзЛрж╕рзНржЯ ржХрж░рж╛ рж╢рзБрж░рзБ рж╣ржЪрзНржЫрзЗ...")
    if not blog_id:
        print("ERROR: BLOG_ID ржкрзВрж░ржг ржХрж░рж╛ рж╣ржпрж╝ржирж┐ред")
        return False
        
    posts_published = []
    
    for post in posts_data:
        post_body = {
            'kind': 'blogger#post',
            'title': post['title'],
            'content': post['content'],
            'labels': post['labels'], 
            'isDraft': False 
        }
        
        try:
            inserted_post = service.posts().insert(blogId=blog_id, body=post_body).execute()
            print(f"      тЬЕ ржкрзЛрж╕рзНржЯ рж╕ржлрж▓ржнрж╛ржмрзЗ ржкрзНрж░ржХрж╛рж╢рж┐ржд рж╣ржпрж╝рзЗржЫрзЗ: {inserted_post['title']}") 
            posts_published.append(post['title'])
            
            time.sleep(POST_DELAY_SECONDS) 
            
        except Exception as e:
            print(f"      тЭМ API ERROR: ржкрзЛрж╕рзНржЯ ржХрж░рж╛рж░ рж╕ржоржпрж╝ ржмрзНржпрж░рзНрже рж╣ржпрж╝рзЗржЫрзЗ: {post['title']}")
            print(f"      тЭМ API ERROR ржмрж┐ржмрж░ржг: {e}")
            
            if 'quotaExceeded' in str(e):
                print("FATAL ERROR: API Quota Limit ржП ржкрзМржБржЫрзЗ ржЧрзЗржЫрзЗржиред 24 ржШржирзНржЯрж╛ ржкрж░рзЗ ржЖржмрж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржиред")
                break 

    return posts_published


# =========================================================
# ржзрж╛ржк рзм: ржорзЗржпрж╝рж╛ржжрзЛрждрзНрждрзАрж░рзНржг ржкрзЛрж╕рзНржЯ ржбрж┐рж▓рж┐ржЯ (ржЯрзНржпрж╛ржЧ-ржнрж┐рждрзНрждрж┐ржХ ржбрж┐рж▓рж┐ржЯ)
# =========================================================

def delete_expired_posts(service, blog_id):
    """ржмрзНрж▓ржЧрзЗрж░ ржЯрзНржпрж╛ржЧржпрзБржХрзНржд ржкрзЛрж╕рзНржЯржЧрзБрж▓рж┐ ржЪрзЗржХ ржХрж░рзЗ ржПржмржВ ржорзЗржпрж╝рж╛ржжрзЛрждрзНрждрзАрж░рзНржг рж╣рж▓рзЗ ржбрж┐рж▓рж┐ржЯ ржХрж░рзЗред"""
    print("\n--- ржзрж╛ржк рзм: ржорзЗржпрж╝рж╛ржжрзЛрждрзНрждрзАрж░рзНржг ржкрзЛрж╕рзНржЯ ржбрж┐рж▓рж┐ржЯ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╢рзБрж░рзБ рж╣ржЪрзНржЫрзЗ (ржЯрзНржпрж╛ржЧ-ржнрж┐рждрзНрждрж┐ржХ) ---")
    
    today = datetime.now().date()
    posts_deleted = 0
    
    try:
        response = service.posts().list(
            blogId=blog_id, 
            labels=SCRAPED_POST_TAG, 
            fetchBodies=False, 
            maxResults=500
        ).execute()
        
        posts = response.get('items', [])
        print(f"тД╣я╕П '{SCRAPED_POST_TAG}' ржЯрзНржпрж╛ржЧржпрзБржХрзНржд ржорзЛржЯ {len(posts)} ржЯрж┐ ржкрзЛрж╕рзНржЯ ржбрж┐рж▓рж┐ржЯрзЗрж░ ржЬржирзНржп ржЪрзЗржХ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗред")
        
        for post in posts:
            post_id = post['id']
            title = post['title']
            labels = post.get('labels', [])
            
            delete_date_str = None
            
            # ЁЯОп WebEndDate ржЯрзНржпрж╛ржЧ ржЦрзЛржБржЬрж╛
            for label in labels:
                if label.startswith(WEB_END_DATE_TAG_PREFIX):
                    date_part = label[len(WEB_END_DATE_TAG_PREFIX):]
                    try:
                        delete_date = datetime.strptime(date_part, '%d-%m-%Y').date()
                        delete_date_str = delete_date.strftime('%Y-%m-%d') 
                        break
                    except ValueError:
                        print(f"      тЪая╕П ржЯрзНржпрж╛ржЧ ржбрзЗржЯ ржкрж╛рж░рзНрж╕рж┐ржВ ржмрзНржпрж░рзНрже: '{label}'")
                        pass

            if delete_date_str:
                if delete_date <= today:
                    print(f"      ЁЯЧСя╕П ржбрж┐рж▓рж┐ржЯ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ: '{title}' (ржбрж┐рж▓рж┐ржЯ ржбрзЗржЯ: {delete_date_str})")
                    service.posts().delete(blogId=blog_id, postId=post_id).execute()
                    posts_deleted += 1
                    time.sleep(DELETE_DELAY_SECONDS) 
                else:
                    print(f"      тД╣я╕П рж╕рзНржХрж┐ржк ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ: '{title}' (ржбрж┐рж▓рж┐ржЯ ржбрзЗржЯ: {delete_date_str} > ржЖржЬржХрзЗрж░ рждрж╛рж░рж┐ржЦ: {today})")
            else:
                print(f"      тЭМ WebEndDate ржЯрзНржпрж╛ржЧ ржЦрзБржБржЬрзЗ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ржирж┐, рж╕рзНржХрж┐ржк ржХрж░рж╛ рж╣рж▓рзЛ: '{title}'")
            
    except Exception as e:
        print(f"тЭМ ржкрзЛрж╕рзНржЯ ржбрж┐рж▓рж┐ржЯ ржХрж░рж╛рж░ рж╕ржоржпрж╝ рждрзНрж░рзБржЯрж┐: {e}")

    print(f"тЬЕ ржбрж┐рж▓рж┐ржЯ ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╕ржорзНржкржирзНржиред ржорзЛржЯ ржбрж┐рж▓рж┐ржЯ рж╣ржпрж╝рзЗржЫрзЗ: {posts_deleted} ржЯрж┐ ржкрзЛрж╕рзНржЯред")


# =========================================================
# ржкрзНрж░ржзрж╛ржи ржлрж╛ржВрж╢ржи (Main Function)
# =========================================================

if __name__ == '__main__':
    print("--- ржзрж╛ржк рзз: Blogger API рж╕рж╛рж░рзНржнрж┐рж╕ рж╕рзЗржЯржЖржк рж╢рзБрж░рзБ рж╣ржЪрзНржЫрзЗ ---")
    blogger_service = get_blogger_service()

    if blogger_service:
        print("тЬЕ Blogger API рж╕рж╛рж░рзНржнрж┐рж╕ рж╕рзЗржЯржЖржк рж╕ржорзНржкржирзНржиред")
        
        # 1. ржкрзНрж░ржержорзЗ ржорзЗржпрж╝рж╛ржжрзЛрждрзНрждрзАрж░рзНржг ржЯрзНржпрж╛ржЧржпрзБржХрзНржд ржкрзЛрж╕рзНржЯржЧрзБрж▓рж┐ ржбрж┐рж▓рж┐ржЯ ржХрж░рж╛ рж╣ржмрзЗ
        print("\n=== ЁЯФД ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ рж╢рзБрж░рзБ: ржкрзНрж░ржержорзЗ ржбрж┐рж▓рж┐ржЯ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ... ===")
        delete_expired_posts(blogger_service, BLOG_ID)
        
        # 2. рждрж╛рж░ржкрж░ ржирждрзБржи ржкрзЛрж╕рзНржЯ рж╕рзНржХрзНрж░рзНржпрж╛ржк, ржлрж┐рж▓рзНржЯрж╛рж░ ржПржмржВ ржкрзНрж░ржХрж╛рж╢ ржХрж░рж╛ рж╣ржмрзЗ
        print("\n=== ЁЯЪА ржкрзНрж░ржХрзНрж░рж┐ржпрж╝рж╛ ржЪрж▓ржорж╛ржи: ржирждрзБржи ржбрзЗржЯрж╛ рж╕ржВржЧрзНрж░рж╣ ржУ ржкрзНрж░ржХрж╛рж╢... ===")
        scrape_filter_and_publish(TARGET_LISTING_URL, blogger_service, BLOG_ID)